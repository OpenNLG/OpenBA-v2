# OpenBA-V2: Reaching 77.3% High Compression Ratio with Fast Multi-Stage Pruning

<p align="center">
 <a href=""><img alt="Code License" src="https://img.shields.io/badge/Code%20License-Apache_2.0-brightgreen.svg"></a>
  <a href=""><img alt="ata License" src="https://img.shields.io/badge/Data%20License-CC%20BY--NC%204.0-blue.svg"></a>
   <a href=""><img alt="Model License" src="https://img.shields.io/badge/Model%20License-GNU%20AGPL%203.0-red.svg"></a>
       <a href=""><img alt="Pretrained Models" src="https://img.shields.io/badge/ğŸ¤— HuggingFace-Pretrained Models-green"></a>
    <a href="https://arxiv.org/abs/2402.16602"><img alt="Paper" src="https://img.shields.io/badge/ğŸ“„-Paper-orange"></a>
    <a href="https://opennlg.cn/"><img src="https://img.shields.io/badge/Organization-OpenNLG%20Group-blueviolet"></a>
</p>
è¿™æ˜¯OpenBA-V2çš„å®˜æ–¹ä»£ç ã€‚OpenBA-V2æ˜¯OpenBAçš„äºŒä»£æ¨¡å‹ï¼Œé€šè¿‡å¤šé˜¶æ®µæ¨¡å‹è£å‰ªè·å¾—ã€‚æ¨¡å‹ä»…æœ‰3.4Bå‚æ•°é‡ï¼Œç›¸è¾ƒäºç¬¬ä¸€ä»£æ¨¡å‹è£å‰ªäº†77.3%çš„æ¨¡å‹å‚æ•°ï¼Œä½†æ‹¥æœ‰æ¥è¿‘çš„æ¨¡å‹æ€§èƒ½ã€‚
æ•´ä¸ªè£å‰ªè¿‡ç¨‹æ¶‰åŠåˆ°layerè£å‰ªï¼Œneuralè£å‰ªï¼Œä»¥åŠè¯è¡¨-embeddingè£å‰ªç­‰å¤šä¸ªæŠ€æœ¯ï¼Œåœ¨æˆ‘ä»¬çš„æŠ€æœ¯æŠ¥å‘Šä¸­æœ‰è¯¦ç»†è¯´æ˜ã€‚

## ç›®å½•
- [å¼€æºè®¡åˆ’](#å¼€æºè®¡åˆ’)
- [è®­ç»ƒè¿‡ç¨‹](#è®­ç»ƒè¿‡ç¨‹)
- [è¯„æµ‹ç»“æœ](#è¯„æµ‹ç»“æœ)
- [ç”¨æ³•](#ç”¨æ³•)
  - [æ¼”ç¤º](#æ¼”ç¤º)
  - [è®­ç»ƒ](#è®­ç»ƒ)
  - [è¯è¡¨è£å‰ª](#è¯è¡¨è£å‰ª)
- [è¯¦ç»†ä¿¡æ¯](#è¯¦ç»†ä¿¡æ¯)
  - [æ¨¡å‹ç»“æ„](#æ¨¡å‹ç»“æ„)
  - [è®­ç»ƒæ•°æ®](#è®­ç»ƒæ•°æ®)
- [å…è´£å£°æ˜](#å…è´£å£°æ˜)


## å¼€æºè®¡åˆ’
æˆ‘ä»¬å°†å¼€æºå¤šä¸ªç‰ˆæœ¬çš„æ¨¡å‹ï¼Œç›®å‰æ­£åœ¨æ•´ç†ä¸­ï¼Œå°†äºè¿‘æœŸä¸Šä¼ ã€‚


## è®­ç»ƒè¿‡ç¨‹
æˆ‘ä»¬é€šè¿‡å¤šé˜¶æ®µçš„æ¨¡å‹è£å‰ªå°†æ¨¡å‹é€æ¸ä»15Bè£å‰ªè‡³3.4Bï¼Œæ¨¡å‹æ¯ä¸ªé˜¶æ®µçš„å‚æ•°ä»¥åŠä½¿ç”¨çš„ç›®æ ‡å‡½æ•°å¦‚ä¸‹ï¼š
| Models | #Params | Enc | Dec | Hidden | FFN | Heads | Data | Objective | Flops $\left(\times 10^{20}\right)$ |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| OpenBA | 15B | 12 | 36 | 4,096 | 11,008 | 40 | 350 B | UL2 | 277.1 |
| Stage1 | 12.3B | 10 | 30 | 4,096 | 11,008 | 40 | 10B | D-UL2 | 6.7 |
| Stage2 | 11.0B | 8 | 27 | 4,096 | 11,008 | 40 | 10B | D-UL2 | 5.9 |
| Stage3 | 9.9B | 8 | 24 | 4,096 | 11,008 | 40 | 15B | D-UL2 | 8.1 |
| Stage4 | 3.8B | 8 | 24 | 2,560 | 6,912 | 20 | 65B | D-UL2 | 13.0 |
| Stage5 | 3.8B | 8 | 24 | 2,560 | 6,912 | 20 | 700B | O-UL2 | 99.1 |
| Prune Emb | 3.4B | 8 | 24 | 2,560 | 6,912 | 20 | - | - | - |

- OpenBAå®¶æ—ä½¿ç”¨çš„æ˜¯ [UL2](https://arxiv.org/pdf/2205.05131.pdf) ä½œä¸ºé¢„è®­ç»ƒç›®æ ‡å‡½æ•°ã€‚åœ¨å‹ç¼©çš„è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬é’ˆå¯¹ä¸åŒçš„é˜¶æ®µå¯¹ UL2 è¿›è¡Œäº†ä¼˜åŒ–ï¼Œåˆ†åˆ«æœ‰ D-UL2 ä»¥åŠ O-UL2 ç­‰å˜ç§ï¼Œåˆ†åˆ«ç”¨äºè§£å†³è£å‰ªåå¤šç§noiseä¸å‡è¡¡ï¼Œä»¥åŠä¼˜åŒ–UL2è®­ç»ƒè¿‡ç¨‹ç§äº§ç”Ÿçš„paddingé—®é¢˜ã€‚
- æ¨¡å‹çš„æ·±åº¦è£å‰ªå‚è€ƒäº†è¿‡å»çš„å·¥ä½œï¼Œé€‰æ‹©è£å‰ªä¸­é—´çš„ï¼Œç›¸éš”è¾ƒè¿œçš„éè¿ç»­å±‚ã€‚æ¨¡å‹å®½åº¦çš„è£å‰ªå‚è€ƒäº† [Sheard-LLaMA](https://github.com/princeton-nlp/LLM-Shearing) ä»¥åŠ [LLM-Pruner](https://github.com/horseee/LLM-Pruner)ï¼ŒæŒ‰ç…§æ¨¡å‹å‰å‘è®¡ç®—çš„ç»“æ„ä¾èµ–å…³ç³»ï¼Œä»¥åŠç›®æ ‡æ¨¡å‹é…ç½®ï¼Œéšæœºè£å‰ªçŸ©é˜µæƒé‡çš„è¡Œå’Œåˆ—ã€‚
- ä»¥ä¸Šæ–¹æ³•æˆ‘ä»¬åœ¨å¼€æºçš„LLaMA2-13Bå’ŒLLaMA2-7Bä¸­ç»§ç»­è¿›è¡Œäº†å®éªŒï¼Œåˆ†åˆ«è£å‰ªäº†æ¨¡å‹50%çš„å‚æ•°ï¼Œå¹¶åœ¨20Bçš„å¼€æºæ•°æ® [The Pile](https://github.com/EleutherAI/the-pile) ä¸Šè¿›è¡Œäº†æ¢å¤è®­ç»ƒï¼Œç»†èŠ‚å¯å‚è€ƒæˆ‘ä»¬çš„å¼€æºä»“åº“ï¼š[Pruning-LLM](https://github.com/jordddan/Pruning-LLMs) 


## è¯„æµ‹ç»“æœ
- æˆ‘ä»¬çš„æ¨¡å‹å¤§å°ä¸º3Bå·¦å³ï¼Œå¹¶ä¸”åœ¨å¼€æºçš„Pileæ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒã€‚å› æ­¤æˆ‘ä»¬ä¸»è¦é€‰å–äº†3Bå·¦å³å¹¶ä¸”åœ¨å¼€æºæ•°æ®ä¸Šè®­ç»ƒçš„æ¨¡å‹è¿›è¡Œæ¯”è¾ƒã€‚
- Qwen, Phi, MiniCPM ç­‰åœ¨é—­æºæ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œæˆ‘ä»¬çš„æ¨¡å‹ä¸è¿™äº›æ¨¡å‹è¿˜å­˜åœ¨ä¸€å®šå·®è·ã€‚
### Few-Shotï¼ŒZero-Shotæµ‹è¯„ç»“æœ
#### çŸ¥è¯†ï¼Œæ•°å­¦èƒ½åŠ›æµ‹è¯„
| Model | #Param. | Avg. | MMLU(5) | BBH(5) | GSM8K(8) | MATH(5) |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: |
| LLaMA2 | 7B | 23.2 | 44.3 | 33.2 | 13.6 | 1.8 |
| Tiny-Ilama | 1.1B | 14.0 | 25.4 | 26.8 | 2.0 | 1.6 |
| Open-Ilama-v2 | 3B | 15.4 | 26.2 | 29.5 | 2.7 | 3.0 |
| Bloom-3B | 3B | 12.4 | 25.8 | 20.5 | 1.6 | 1.7 |
| MindLLM | 1.3B | 9.8 | 25.4 | 9.9 | 2.3 | 1.4 |
| GPT-NEO | 2.7B | 14.0 | 25.0 | 26.7 | 2.2 | 1.9 |
| INCITE-Base-3B | 3B | 14.5 | 27.0 | 27.0 | 2.1 | 1.7 |
| Sheard-LLama | 2.7B | 15.4 | 27.1 | 29.2 | 2.4 | 2.8 |
| Qwen | 1.8B | 25.5 | 45.3 | 22.3 | 32.3 | 2.3 |
| OpenBA | 15B | 26.6 | 40.2 | 34.1 | 17.2 | 14.9 |
| OpenBA-v2 | 3.8B | 23.9 | 37.9 | 29.5 | 13.5 | 14.8 |
| v2-Emb-Pruned | 3.4B | 24.0 | 37.9 | 29.6 | 13.6 | 14.8 |

#### ä¸­æ–‡èƒ½åŠ›æµ‹è¯„
| Model | #Param. | Avg. | C-EVAL(5) | CMMLU(5) |
| :--- | :---: | :---: | :---: | :---: |
| Chinese-LLaMA2 | 7B | 34.5 | 35.9 | 33.0 |
| Bloom-3B | 3B | 25.3 | 25.4 | 25.2 |
| MindLLM | 1.3B | 26.9 | 28.0 | 25.7 |
| Qwen | 1.8B | 54.1 | 56.1 | 52.1 |
| Mini-cpm | 2.7 B | 51.1 | 51.1 | 51.1 |
| Phi2 | 2.7B | 33.8 | 35.1 | 32.5 |
| OpenBA | 15B | 40.7 | 39.8 | 41.5 |
| OpenBA-v2 | 3.8B | 38.3 | 37.8 | 38.8 |
| v2-Emb-Pruned | 3.4B | 38.1 | 37.3 | 38.9 |

#### QAæµ‹è¯„
| Model | #Param. | AVG. | SciQ | PIQA | ARC-E | ARC-C | LogiQA | BoolQ |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| LLaMA2 | 7B | 69.0 | 93.7 | 78.1 | 76.4 | 53.0 | 30.7 | 82.1 |
| Baichuan2 | 7B | 67.4 | 94.7 | 78.4 | 78.9 | 49.7 | 29.3 | 73.2 |
| ChatGLM3 | 7B | 66.3 | 94.4 | 73.5 | 68.6 | 52.3 | 30.3 | 78.7 |
| Tiny-LLaMA | 3B | 56.1 | 94.0 | 73.3 | 55.3 | 30.1 | 26.3 | 57.8 |
| Open-LLaMA-v2 | 3B | 61.9 | 91.8 | 76.2 | 66.5 | 39.0 | 28.1 | 69.6 |
| Bloom-3B | 3B | 59.2 | 93.5 | 70.7 | 64.2 | 35.2 | 29.3 | 62.1 |
| MindLLM | 1.3B | 49.5 | 80.2 | 64.9 | 47.1 | 24.8 | 28.0 | 52.1 |
| GPT-NEO | 2.7B | 59.7 | 93.3 | 74.2 | 65.3 | 35.2 | 28.4 | 61.8 |
| INCITE-Base-3B | 3B | 61.1 | 90.7 | 74.6 | 67.7 | 40.2 | 27.7 | 65.9 |
| Sheard-LLama | 2.7B | 62.9 | 90.8 | 75.8 | 67.0 | 41.2 | 28.9 | 73.7 |
| Qwen | 1.8B | 61.1 | 92.2 | 73.6 | 63.7 | 38.5 | 31.6 | 66.8 |
| OpenBA | 15B | 67.8 | 94.6 | 71.9 | 69.7 | 54.4 | 33.3 | 82.6 |
| OpenBA-v2 | 3.8B | 65.1 | 94.7 | 71.3 | 63.7 | 47.7 | 32.1 | 81.3 |
| v2-Emb-Pruned | 3.4B | 65.1 | 94.6 | 70.8 | 63.6 | 47.7 | 32.3 | 81.4 |

### ä¸‹æ¸¸ä»»åŠ¡å¾®è°ƒçš„æµ‹è¯•ç»“æœ
æˆ‘ä»¬è¿›ä¸€æ­¥æ¢ç´¢äº†OpenBAæ¨¡å‹åœ¨å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰åº”ç”¨æ–¹é¢çš„æ½œåŠ›ã€‚
ä¸ºäº†è®­ç»ƒç›®çš„ï¼Œæˆ‘ä»¬åˆ©ç”¨äº† [Pile-NER](https://arxiv.org/abs/2311.09122) æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«å¤§çº¦240,000ä¸ªå®ä½“ï¼Œæ¶µç›–äº†13,000ä¸ªä¸åŒç±»åˆ«çš„å®ä½“ã€‚
éšåï¼Œæˆ‘ä»¬ä½¿ç”¨ MIT å’Œ CrossNER æ•°æ®é›†è¯„ä¼°äº†æˆ‘ä»¬æ¨¡å‹OpenBA-v2-NERçš„æ€§èƒ½ï¼Œå…¶ä¸­å®ä½“æ ‡ç­¾åœ¨è®­ç»ƒé˜¶æ®µåŸºæœ¬ä¸Šæ˜¯æœªè§è¿‡çš„ã€‚
ç›‘ç£å¾®è°ƒçš„promptä»¥åŠè®­ç»ƒæ–¹æ³•å‚è€ƒ [GNER](https://arxiv.org/abs/2402.16602)ã€‚
ç»“æœæ˜¾ç¤ºæˆ‘ä»¬çš„æ¨¡å‹æ˜æ˜¾ä¼˜äºå…¶ä»–æ¨¡å‹ï¼Œå®ç°äº†è¶…è¶ŠåŸå§‹15Bæ¨¡å‹ä¿®å‰ªä¹‹å‰çš„å“è¶Šæ€§èƒ½ã€‚
| Model | Movie | Restaurant | AI | Literature | Music | Politics | Science | Avg |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| Sheard-LLaMA | 29.4 | 15.7 | 34.5 | 33.7 | 39.7 | 33.3 | 35.8 | 31.7 |
| Open-LLaMA-v2 | 63.0 | 44.3 | 58.9 | 50.2 | 72.0 | 66.4 | 62.5 | 59.6 |
| OpenBA-NER-15B | 51.4 | 37.4 | 58.5 | 54.6 | 69.1 | 68.4 | 68.2 | 58.2 |
| OpenBA-v2-NER-3B | 60.3 | 43.3 | 63.8 | 54.3 | 72.4 | 67.4 | 70.6 | 61.7 |

## ç”¨æ³•
### æ¼”ç¤º 
é¦–å…ˆï¼Œä½ éœ€è¦å®‰è£…ä»¥ä¸‹çš„ä¾èµ–ï¼š
```bash
pip install transformers==4.31.0 torch>=2.0 sentencepiece
```
```æ³¨æ„ï¼š``` è¯·ç¡®ä¿æ‚¨çš„transformersåº“ç‰ˆæœ¬ä¸è¶…è¿‡4.33.2 ï¼

UL2ä½¿ç”¨äº†ç‰¹æ®Štoken `<S>` å’Œç‰¹æ®Štoken `<extra_id_0>` ï¼Œæ‰€ä»¥ä½ å¯ä»¥å°†è¾“å…¥æŒ‡ä»¤æ ¼å¼åŒ–ä¸º `<S> {your input} <extra_id_0>` ä»¥è·å¾—æ›´å¥½çš„ç­”æ¡ˆã€‚


### è®­ç»ƒ
æˆ‘ä»¬çš„è®­ç»ƒä»£ç åœ¨`training`æ–‡ä»¶å¤¹ä¸­ã€‚åŸºäº[Megatron-LM](https://github.com/NVIDIA/Megatron-LM/)ï¼Œæˆ‘ä»¬è¿›è¡Œäº†ä»¥ä¸‹å®ç°ï¼š

å¯¹äºé¢„è®­ç»ƒï¼Œåº”äº‹å…ˆå®‰è£…ç›¸å…³ä¾èµ–ï¼Œå¦‚[Megatron-LM](https://github.com/NVIDIA/Megatron-LM/)ä¸­æ‰€è¿°ï¼Œç„¶åæ‚¨å¯ä»¥ç®€å•åœ°è¿è¡Œä»¥ä¸‹å‘½ä»¤å°†æ–‡æœ¬å¤„ç†ä¸ºå­—èŠ‚ï¼Œè¿™æ ·MMapæ•°æ®é›†å¯ä»¥æ›´å¿«åœ°è¯»å–ï¼š

```bash
cd training
bash scripts/data_process_span_corr.sh  # å¤„ç†é¢„è®­ç»ƒæ•°æ®
bash scripts/data_process_flan.sh  # å¤„ç†å¾®è°ƒæ•°æ®
```

æ‚¨å¯ä»¥é€šè¿‡ä»¥ä¸‹å‘½ä»¤åœ¨å¤šä¸ªèŠ‚ç‚¹ä¸Šè¿è¡Œåˆ†å¸ƒå¼è®­ç»ƒ
```bash
bash scripts/train_d_ul2.sh  # ä½¿ç”¨ d_ul2 æ–¹æ³•è¿›è¡Œå‡è¡¡denoiseè®­ç»ƒ
bash scripts/train_o_ul2.sh  # ä½¿ç”¨ o_ul2 è®­ç»ƒ 
bash scripts/prune_model.sh   # å°†æ¨¡å‹æ£€æŸ¥ç‚¹è£å‰ªè‡³ä»»æ„æŒ‡å®šé…ç½® 
```

å…¶ä»–çš„ä¸€äº›å·¥å…·è„šæœ¬
```bash
bash scripts/split_model.sh  # è½¬æ¢æ¨¡å‹çš„å¼ é‡å¹¶è¡Œå¤§å°
bash convert_megatron_to_hf_ckpt.sh # megatronæ¨¡å‹è½¬åŒ–ä¸ºhfæ¨¡å‹
```

### è¯è¡¨è£å‰ª
æ¨¡å‹è¯è¡¨è£å‰ªä¹Ÿæ˜¯æœ¬é¡¹ç›®çš„ä¸€ä¸ªäº®ç‚¹ï¼Œä»…è£å‰ªä¸å¸¸ç”¨çš„è¯è¡¨ï¼Œå¯ä»¥è£å‰ªè¿‘10%çš„å‚æ•°è€Œå®Œå…¨ä¸æŸå®³æ¨¡å‹æ€§èƒ½ã€‚æˆ‘ä»¬åšäº†åˆ†æå®éªŒï¼Œä¸€èˆ¬æ¥è¯´ï¼Œä¸­æ–‡è¯éœ€æ±‚çš„è¯è¡¨è¾ƒå°‘ï¼Œè€Œè‹±æ–‡éœ€è¦çš„æ¨¡å‹è¯è¡¨è¾ƒå¤šï¼Œå®éªŒç»“æœå¦‚ä¸‹

<center>
<div align=center><img width="500" src="assets/emb.png"/></div>
</center>

è¯è¡¨è£å‰ªä»£ç ä»¥åŠä½¿ç”¨è¯´æ˜è¯·å‚è€ƒ [è¿™é‡Œ]("https://github.com/jordddan/OpenBA-v2/blob/main/training/prune_vocab_emb/README.md").

## å…è´£å£°æ˜
ä½¿ç”¨OpenBA-LMåº”éµå¾ªç¤¾ä¼šè§„èŒƒï¼Œä¸å¾—ç”¨äºå±å®³å›½å®¶æˆ–ç¤¾ä¼šå®‰å…¨æˆ–è¿åæ³•å¾‹çš„æ´»åŠ¨ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¦æ±‚ç”¨æˆ·ä¸è¦å°†OpenBA-LMç”¨äºå°šæœªç»è¿‡é€‚å½“å®‰å…¨å®¡æŸ¥å’Œè®°å½•çš„äº’è”ç½‘æœåŠ¡ã€‚æˆ‘ä»¬å¸Œæœ›æ‰€æœ‰ç”¨æˆ·éƒ½éµå®ˆè¿™ä¸€åŸåˆ™ï¼Œç¡®ä¿æŠ€æœ¯å‘å±•åœ¨ä¸€ä¸ªæœ‰åºã€åˆæ³•çš„ç¯å¢ƒä¸­è¿›è¡Œã€‚

æˆ‘ä»¬å·²å°½æœ€å¤§åŠªåŠ›ç¡®ä¿æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­ä½¿ç”¨çš„æ•°æ®ç¬¦åˆè§„å®šã€‚ç„¶è€Œï¼Œå°½ç®¡æˆ‘ä»¬ä»˜å‡ºäº†å·¨å¤§çš„åŠªåŠ›ï¼Œä½†ç”±äºæ¨¡å‹å’Œæ•°æ®çš„å¤æ‚æ€§ï¼Œä»å¯èƒ½å‡ºç°æ„å¤–é—®é¢˜ã€‚å¦‚æœåœ¨æä¾›æœåŠ¡è¿‡ç¨‹ä¸­ï¼Œé€šè¿‡ä½¿ç”¨æœ¬é¡¹ç›®ä¸­åŒ…å«çš„æ¨¡å‹æˆ–å…¶ä¿®æ”¹ç‰ˆæœ¬ç”Ÿæˆè¯¯å¯¼æ€§æˆ–æœ‰å®³çš„é™ˆè¿°ï¼Œè´£ä»»åœ¨äºæœåŠ¡æä¾›å•†ï¼Œä¸æœ¬é¡¹ç›®æ— å…³ã€‚

## å¼•ç”¨
å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®å¯¹æ‚¨æœ‰å¸®åŠ©ï¼Œè¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼å¼•ç”¨ï¼š
```
@article{li2023openba,
  title={OpenBA: An Open-sourced 15B Bilingual Asymmetric seq2seq Model Pre-trained from Scratch},
  author={Li, Juntao and Tang, Zecheng and Ding, Yuyang and Wang, Pinzheng and Guo, Pei and You, Wangjie and Qiao, Dan and Chen, Wenliang and Fu, Guohong and Zhu, Qiaoming and others},
  journal={arXiv preprint arXiv:2309.10706},
  year={2023}
}
```